\chapter{Návrh}  % chapter* je necislovana kapitola
\label{navrh}

Na základe poznatkov získaných z analýzy reziduálnych sieti sme zistili, že reziduálne siete sa osvedčili ako vhodný prostriedok na aproximáciu identického zobrazenia. Tieto poznatky boli overené na základe rôznych experimentov \cite{Targ2016, He2016, Wu2017}. Štandardné reziduálne siete sú trénované klasickým algoritmom spätného šírenia chyby.

Trénovanie akejkoľvek neurónovej siete pomocou algoritmu spätného šírenia chyby má nevýhodu neefektívneho pristupovania k jednotlivým zdrojom danej neurónovej siete (\ref{synth_grad}). Naším cieľom je realizovať trénovanie neurónovej siete využitím algoritmu syntetického gradientu a tým odstrániť spätné šírenie chyby z procesu trénovania neurónovej siete. Algoritmus syntetického gradientu je použitý pri trénovaní reziduálnej siete. Vybraná reziduálna sieť vykonáva kalibráciu cytometrických dát pri gatovaní krvných vzoriek metódou DeepCyTOF.

Pri realizácii sa zameriame na úpravu existujúcej implementácie metódy DeepCyTOF. Metóda DeepCyTOF bude použitá na klasifikáciu krvných vzoriek cytometrickej dátovej sady ktorou disponuje fakulta. Pred začatím implementácie je potrebné danú cytometrickú dátovú sadu konvertovať na vhodný formát (napr. csv). Konvertovanie dátovej sady je realizovateľné pomocou existujúcich programov vyvinutých študentami fakulty, ktorí sa venovali spracovaniu cytometrických dát.

Úprava existujúcej implementácie metódy DeepCyTOF zahŕňa predovšetkým úpravu procesu trénovania MMD-ResNet. Trénovanie MMD-ResNet bude vykonávať úpravu váh jednotlivých vrstiev využitím syntetického gradientu. Syntetický gradient bude generovaný modulmi syntetického gradientu (rovnakou metódou aká je uvedená v Kapitole \ref{SGaDNI}). V celom modeli reziduálnej siete sa bude nachádzať celkov $I-1$ modulov syntetického gradientu, kde $I$ predstavuje celkový počet všetkých vrstiev MMD-ResNet (vrátane vstupnej a výstupnej vrstvy). Moduly syntetického gradientu budú šíriť syntetický gradient všetkým vrstvám s výnimkou výstupnej vrstvy. Výstupná vrstva bude realizovať úpravu svojich váh pomocou skutočného gradientu získaného na základe chybovej funkcie $L(w) = \sqrt{MMD^2(f(X),Y)}$, kde $f(X)$ predstavuje výstupnú hodnotu reziduálnej siete (transformované identické zobrazenie $X$), $Y$ je supervízia (referenčná vzorka) a $w$ sú parametre reziduálnej siete.

% Použitie synt. grad. v reziduálnych sieťach

V kapitole \ref{residual_learning} sme uviedli, že identitné prepojenie nepridáva žiadne dodatočné parametre (váhy) trénovanému modelu. Vzhľadom k tomu, že úprava váh je realizovaná len v rámci kumulovaných vrstiev reziduálnych blokov, tak syntetický gradient je potrebný poskytovať len týmto vrstvám. V prípade použitia algoritmu spätného šírenia chyby je skutočný gradient šírený reziduálnou sieťou ako opisuje Obrázok \ref{fig:ResNetWithSG} a). Implementáciou syntetického gradientu je spätné šírenie chyby odstránené z procesu trénovania reziduálnej siete. Šírenie syntetického gradientu reziduálnou sieťou je opísané Obrázkom \ref{fig:ResNetWithSG} b).

% Obrázok implementácie
\begin{figure}

%vlozenie samotneho obrazku vycentrovaneho a vhodnej velkosti
%obrazok je v subore images/cervik.png
\centerline{\includegraphics[width=1.0\textwidth]{images/ResNet_with_SG.png}}
%popis obrazku
\caption[Reziduálna sieť s implementovaným algoritmom syntetického gradientu]{Porovnanie architektúr ResNet s využitím algoritmu spätného šírenia chyby (a) a algoritmu syntetického gradientu (b). a) Architektúra ResNet ktorá v rámci trénovania využíva algoritmus spätného šírenia chyby. Plné šípky znázorňujú tok informácií v rámci doprednej transformácie. Prerušované šípky znázorňujú tok gradientu reziduálnou sieťou. b) Architektúra ResNet s využitím algoritmu syntetického gradientu. Plné šípky znázorňujú tok informácií v rámci doprednej transformácie. Čierne prerušované šípky znázorňujú tok gradientu poskytovaného jednotlivým vrstvám na úpravu váh. Červené prerušované šípky znázorňujú tok gradientu poskytovaného modulom syntetického gradientu na úpravu ich váh. Z obrázka b) je možné vidieť, že všetky vrstvy s výnimkou výstupnej (poslednej) využívajú syntetický gradient generovaný modulmi syntetického gradientu. Výstupná vrstva využíva skutočný gradient získaný z výstupu neurónovej siete.}
\label{fig:ResNetWithSG}
%id obrazku, pomocou ktoreho sa budeme na obrazok odvolavat
\end{figure}


Architektúra reziduálneho bloku bude zachovaná z pôvodnej implementácie (viď Obrázok \ref{MMD-ResNet_arch}). Reziduálny blok pozostáva z dvoch kumulovaných vrstiev, ktoré sekvenčne vykonávajú dávkovú normalizáciu, transformáciu dát pomocou ReLU a finálne váhovanie. Na výstupe jednotlivých reziduálnych blokov nie je vykonávaná žiadna dodatočná nelineárna transformácia.

Architektúra identitného prepojenia bude ponechaná z pôvodnej implementácie.
Identitné prepojenie nebude vykonávať žiadnu dodatočnú transformáciu identického zobrazenia šíreného týmto identitným prepojením. Táto implementácia sa na základe výsledkov experimentov opísaných v Kapitole \ref{implementation}, ukázala ako najoptimálnejšia možnosť. Reziduálna sieť využívajúca takúto architektúru identitného prepojenia dosahovala najvyššiu hodnotu presnosti v porovnaní s rezidualnými sieťami, ktoré využívali iné architektúry identitného prepojenia.

% opísať ten shit z technických detailov ktoré som vynechal v analýze

Pri zmene váh jednotlivých vrstiev je použitý dynamický krok učenia. Dynamickosť kroku učenia je realizovaná rozvrhom. Krok učenia je podobne ako v \cite{Li2017}, definovaný ako 
\begin{equation}
\label{dyna_lr}
    \alpha_t = \alpha_0 . \gamma^{\lfloor\frac{1+t}{T}\rfloor}
\end{equation}
kde $\alpha_t$ je hodnota kroku učenia v iterácii učenia $t$, $\alpha_0$ je počiatočná hodnota kroku učenia, $\gamma$ je predom definovaná konštanta a $T$ je zvolený rozvrh. Takto navrhnutý, dynamicky sa meniaci krok učenia pomáha zjemňovať konvergenciu presnosti neurónovej siete. V neskorších fázach trénovania je preto prírastok presnosti neurónovej siete nižší. Implementácia dynamického kroku učenia bráni pretrénovaniu neurónovej siete.

Automatizovanie vyhodnocovania výsledkov generatívnych modelov je pomerne náročnejšie ako automatizovanie vyhodnocovania výsledkov diskriminatívnych modelov \cite{Bethge2016}. Jeden zo spôsobov vyhodnocovania presnosti MMD-ResNet je meranie vzdialenosti distribúcií hodnôt kalibrovanej vzorky voči referenčnej vzorke. Tento prístup však nemusí nutne poskytovať užitočné výsledky. Vyhodnocovanie presnosti MMD-ResNet bude realizované meraním vzdialenosti distribúcií hodnôt vzorky kalibrovanej pomocou MMD-ResNet s využitím algoritmu syntetického gradientu voči tej istej vzorke kalibrovanej pomocou MMD-ResNet s využitím algoritmu spätného šírenia chyby. Týmto meraním vieme následne porovnať rozdielnosť výsledkov kalibrácií ktoré boli realizované MMD-ResNet s rozdielnými trénovacími algoritmami.

Smerodajnou hodnotou presnosti MMD-ResNet s využitím algoritmu syntetického gradientu bude vyhodnotenie vplyvu tejto reziduálnej siete na presnosť metódy DeepCyTOF ako celku. Presnosť metódy DeepCyTOF využívajúcej MMD-ResNet na kalibráciu dát bude vyhodnocovaná na základe F1 skóre. F1 skóre predstavuje štatistický test vyhodnocujúci binárne hodnoty štatistických výsledkov \cite{sasaki2007truth}. Vzhľadom k tomu, že výstupom metódy DeepCyTOF sú kategorické hodnoty, tak vyhodnotenie F1 skóre je potrebné upraviť do takej podoby, aby ním bolo možné vyhodnocovať aj nebinárne množiny. F1 skóre na vyhodnocovanie kategorických hodnôt je definované ako vážený priemer F1 skóre pre každú bunkovú kategóriu samostatne, 
\begin{equation}
    F=\sum_i \frac{c_i}{N}F_i
\end{equation}
kde $c_i$ je počet buniek kategórie $i$, $N$ je celkový počet buniek v krvnej vzorke a $F_i$ je F1 skóre pre $i$-tu kategóriu buniek voči všetkým ostatným bunkám \cite{Li2017}. F1 skóre $F_i$ pre $i$-tu kategóriu je definované ako 
\begin{equation}
F_i=2\frac{P \times R}{P + R}
\end{equation}
kde $P$ je podiel počtu správne klasifikovaných pozitívnych hodnôt a počtu všetkých pozitívnych hodnôt predikovaných klasifikátorom a $R$ je podiel počtu správne klasifikovaných pozitívnych hodnôt a celkového počtu všetkých skutočných pozitívnych hodnôt danej kategórie \cite{sasaki2007truth}.

Alternatívnou metrikou vyhodnocovania je pozorovanie vývoja hodnoty MMD počas trénovania MMD-ResNet s využitím algoritmu syntetického gradientu. Týmto spôsobom je možné lepšie pochopiť správanie vplyvu syntetického gradientu na proces trénovania MMD-ResNet. Pozorovaný vývoj hodnoty MMD je následne možné porovnať s vývojom hodnoty MMD, získaným trénovaním MMD-ResNet s využitím metódy štandardného spätného šírenia chyby.

Pri pozorovaní vývoja hodnoty MMD v procese trénovania MMD-ResNet berieme do úvahy, v akom čase sú dosiahnuté jednotlivé hodnoty MMD. Na základe teoretických znalostí je možné preukázať, že paralelizácia trénovania jednotlivých vrstiev MMD-ResNet vedie k zníženiu výpočtového času potrebného na realizáciu daného trénovania \cite{Jaderberg2016, Czarnecki2017}. Vzhľadom k tomu, že jednotlivé algoritmy trénovania MMD-Resnet (algoritmus syntetického gradientu a štandardné spätné šírenie chyby) môžu mať odlišnú mieru konvergencie hodnoty MMD, tak je potrebné brať do úvahy aj počet iterácií, ktoré sú potrebné na dosiahnutie jednotlivých hodnôt MMD.

\chapter{Zhodnotenie}

V Kapitole \ref{SGzhrnutie} sme uviedli, že neurónové siete trénované využitím algoritmu syntetického gradientu dokážu lepšie konvergovať za predpokladu, že pozostávajú z viacerých skrytých vrstiev. Výsledky experimentov ktoré skúmali implementáciu syntetického gradientu v hlbokých neurónových sieťach nepreukázali žiaden signifikantný pokles presnosti skúmaných neurónových sieti. Na základe týchto tvrdení sa domnievame, že využitie algoritmu syntetického gradientu v procese trénovania reziduálnej siete by malo mať podobný efekt. 

Súčasné experimenty sa zaoberajú implementáciou syntetického gradientu predovšetkým v rekurentných, konvolučných a jednoduchých dopredných neurónových sieťach. Pre implementáciu syntetického gradientu v reziduálnych sieťach sme sa rozhodli z dôvodu, že v súčasnosti nie je zdokumentovaný žiaden experiment pri ktorom by bol syntetický gradient použitý na trénovanie reziduálnej siete. 

Po dôkladnej analýze sme zistili, že algoritmus syntetického gradientu je možné využiť pri trénovaní akýchkoľvek modelov neurónových sieti. Skúmanie vplyvu syntetického gradientu na presnosť reziduálnych sieti môže mať značný prínos v oblasti strojového učenia. Výsledky pozorovaní je možné ďalej aplikovať aj v iných oblastiach strojového učenia a na iné modely neurónových sieti.

% spomenuť, že počas analýzy sme nenašli žiadne existujúce riešenie synt. gradient + reziduálna sieť