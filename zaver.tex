\chapter*{Záver}  % chapter* je necislovana kapitola
\addcontentsline{toc}{chapter}{Záver} % rucne pridanie do obsahu
\markboth{Záver}{Záver} % vyriesenie hlaviciek

Ako sme už mnohokrát spomenuli, cieľom tejto práce je odpozorovať vplyv implementácie algoritmu syntetického gradientu na priebeh trénovania neurónovej siete. S myšlienkou implementácie jednoduchých neurónových sieti trénovaných na predikciu hodnoty gradientu prišla organizácia DeepMind, divízia spoločnosti Google orientovaná vo výskume umelej inteligencie. V súčastnosti bol algoritmus syntetického gradientu experimentálne integrovaný predovšetkým v jednoduchých dopredných neurónových sieťach, rekurentných neurónových sieťach a konvolučných neurónových sieťach. Vzhľadom k tomu, že doposiaľ sa nepodarilo nájsť žiaden zdokumentovaný experiment, kde by bol algoritmus syntetického gradientu implementovaný v reziduálnych sieťach, tak sme sa rozhodli pozorovať vplyv implementácie syntetického gradientu práve v reziduálnych sieťach. Zamerali sme sa predovšetkým na nahradenie pôvodnej rezdiduálnej siete MMD-ResNet použitej na kalibráciu vzoriek pri automatickom gatovaní metódou DeepCyTOF.

Ako sme uviedli v Kap. \ref{vysledky}, menšími úpravami metódy DeepCyTOF a ladením hyperparametrov MMD-ResNet sa nám podarilo dosiahnuť konkurencieschopné výsledky v porovnaní s pôvodnou implementáciou MMD-ResNet. Výsledky ktoré boli získané pri jednotlivých experimentoch boli vopred očakávané. Nižšia miera konvergencie hodnoty MMD pri trénovaní MMD-ResNet využívajúcej algoritmus syntetického gradientu je zapríčinená práve aproximáciou skutočného gradientu (viď Kap. \ref{vyvoj_MMD_v_iteraciach}). V tomto prípade sa miera konvergencie odvíja práve od toho, že jednotlivé skryté vrstvy MMD-ResNet nie sú trénované skutočným gradientom ale jeho aproximáciou, ktorá nie je úplne presná. 

Vzhľadom k tomu, že implementácia modulov syntetického gradientu umožňuje paralelné trénovanie jednotlivých vrstiev MMD-ResNet, tak je zabezpečené zníženie času potrebného na trénovanie MMD-ResNet. Na základe výsledkov experimentu uvedených v Kap. \ref{vyvoj_MMD_v_case}, je možné vidieť že model MMD-ResNet využívajúci algoritmus syntetického gradientu dokončil trénovanie skôr ako model MMD-ResNet využívajúci algoritmus spätného šírenia chyby, pričom obidve trénovania boli realizované 300 iteráciami. MMD-ResNet využívajúca algoritmus syntetického gradientu dokončila trénovanie po 121 sekundách, zatiaľ čo MMD-ResNet využívajúca algoritmus spätného šírenia chyby dokončil trénovanie po 205 sekundách. V tomto prípade je dôležité zamerať sa na bod, kedy jednotlivé modely dosiahli optimálnu hodnotu MMD. Optimálna hodnota MMD predstavuje hodnotu po ktorej dosiahnutí je model ďalším trénovaním už pretrénovaný, resp. nie je dosiahnutý signifikantný pokles hodnoty MMD. Trénovanie MMD-ResNet využívajúcej algoritmus spätného šírenia chyby trvalo síce 205 sekúnd ale optimálna hodnota MMD bola dosiahnutá už po 95 sekundých, čo je zabezpečené práve vyššou mierou konvergencie oproti MMD-ResNet využívajúcej algoritmus syntetického gradientu, ktorej trénovanie trvalo síce 121 sekúnd ale optimálna hodnota bola dosiahnutá až po 90 sekundách. V tomto prípade 5 sekundový rozdiel nie je signifikantný ale v prípade trénovania MMD-ResNet na väčšom množstve iterácií, rozdiel v čase dosiahnutia optimálnej hodnoty MMD narastá lineárne.

Smerodajná metrika vyhodnotenia presnosti kalibrácie MMD-ResNet je \textit{F1 skóre} dosiahnuté pri automatickom gatovaní kalibrovaných vzoriek. Pomocou jednoduchej funkcie ladenia hyperparametrov sa nám pri automatickom gatovaní vzoriek kalibrovaných pomocou MMD-ResNet využívajúcej algoritmus syntetického gradientu podarilo dosiahnuť \textit{F1 skóre} až 0,836. Zdroj \cite{Li2017} uviedol, že pôvodná implementácia metódy DeepCyTOF zabezpečuje \textit{F1 skóre} až 0,92. Na základe naších výsledkov je možné vyjadriť signifikantný pokles hodnoty \textit{F1 skóre} pri automatickom gatovaní vzoriek kalibrovaných pomocou MMD-ResNet využívajúcej algoritmus syntetického gradientu. 

Domnievame sa, že daný problém môže byť zapríčinený náhodnou inicializáciou váh a parametrov jednotlivých skrytých vrstiev MMD-ResNet v korelácii s náhodnou inicializáciou váh a parametrov vrstiev modulov syntetického gradientu. Vzhľadom k tomu, že naša implementácia zahŕňa viacej prípadov náhodného inicializovania práve z dôvodu inicializovania váh modulov syntetického gradientu, tak náhodný faktor sa v tomto prípade prejavuje vo väčšej miere ako pri pôvodnej implementácii. Vplyv na pokles \textit{F1 skóre} môže mať aj chybová funkcia MMD na výstupe MMD-ResNet. Funkcia MMD vypočítava hodnotu MMD medzi bunkami obsiahnutými v trénovacej dávke a náhodne vybranými bunkami z referenčnej vzorky. Vzhľadom k tomu, že výber buniek z referenčnej vzorky je náhodný, tak výsledok funkcie je ovplyvnený istým stochastickým faktorom. Tento faktor má priamy vplyv na hodnotu gradientu vypočítaného z výsledku chybovej funkcie, ktorý je následne použitý na výpočet gradientu pre jednotlivé moduly syntetického gradientu.

Výsledky implementácie algoritmu syntetického gradientu v MMD-ResNet boli získavané v komplexnejšom prostredí ako uvádzajú zdroje \cite{Jaderberg2016, Czarnecki2017}. Zdroje uvádzajú implementáciu algoritmu syntetického gradientu v prostrediach, kde presnosť pozorovaného modelu nie je podmienená výsledkami iného modelu. V našom prípade je MMD-ResNet trénovaná za účelom znižovania hodnoty MMD medzi zdrojovými vzorkami a referenčnou vzorkou ale smerodajný ukazovateľ presnosti MMD-ResNet je výsledok automatického gatovania vzoriek kaliborvaných pomocou tohto modelu. Nie je možné ovplyvniť trénovanie MMD-ResNet tak, aby pri trénovaní bola braná do úvahy aj hodnota MMD a aj hodnota \textit{F1 skóre} získaná pri klasifikácii kalibrovaných vzoriek v danej iterácii trénovania MMD-ResNet. Domnievame sa, že lepšie výsledky je možné dosiahnúť dôkladnejším ladením hyperparametrov.