Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Li2016,
abstract = {Mass cytometry or CyTOF is an emerging technology for high-dimensional multiparameter single cell analysis that overcomes many limitations of fluorescence-based flow cytometry. New methods for analyzing CyTOF data attempt to improve automation, scalability, performance, and interpretation of data generated in large studies. However, most current tools are less suitable for routine use where analysis must be standardized, reproducible, interpretable, and comparable. Assigning individual cells into discrete groups of cell types (gating) involves time-consuming sequential manual steps untenable for larger studies. The subjectivity of manual gating introduces variability into the data and impacts reproducibility and comparability of results, particularly in multi-center studies. The FlowCAP consortium was formed to address these issues and it aims to boost user confidence in the viability of automated gating methods. We introduce DeepCyTOF, a standardization approach for gating based on a multi-autoencoder neural network. DeepCyTOF requires labeled cells from only a single sample. It is based on domain adaptation principles and is a generalization of previous work that allows us to calibrate between a source domain distribution (reference sample) and multiple target domain distributions (target samples) in a supervised manner. We apply DeepCyTOF to two CyTOF datasets generated from primary immune blood cells: (i) 14 subjects with a history of infection with West Nile virus (WNV), and (ii) 34 healthy subjects of different ages. Each blood sample was labeled with 42 antibody markers, 12 of which were used in our analysis, at baseline and three different stimuli (PMA/ionomycin, tumor cell line K562, and infection with WNV). In each of these datasets we manually gated a single baseline reference sample to automatically gate the remaining uncalibrated samples. We show that DeepCyTOF cell classification is highly concordant with cell classification obtained by individual manual gating of each sample with over 99{\%} concordance. Additionally, we apply a stacked autoencoder, which is one of the building blocks of DeepCyTOF, to cytometry datasets used in the 4th challenge of the FlowCAP-I competition and demonstrate that it over performs relative to all gating methods introduced in this competition. We conclude that stacked autoencoders combined with a domain adaptation procedure offers a powerful computational approach for semi-automated gating of CyTOF and flow cytometry data such that manual gating of one reference sample is sufficient for accurately gating the remaining samples.},
author = {Li, Huamin and Shaham, Uri and Yao, Yi and Montgomery, Ruth and Kluger, Yuval},
doi = {10.1101/054411},
file = {:Users/lukashanincik/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2016 - DeepCyTOF Automated Cell Classification of Mass Cytometry Data by Deep Learning and Domain Adaptation.pdf:pdf},
journal = {bioRxiv},
pages = {054411},
title = {{DeepCyTOF: Automated Cell Classification of Mass Cytometry Data by Deep Learning and Domain Adaptation}},
url = {https://www.biorxiv.org/content/early/2016/05/20/054411},
year = {2016}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, Wojciech Marian and {\'{S}}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
doi = {10.1103/PhysRevB.83.045303},
eprint = {1703.00522},
file = {:Users/lukashanincik/Library/Application Support/Mendeley Desktop/Downloaded/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
url = {http://arxiv.org/abs/1703.00522},
year = {2017}
}
@article{Ioffe2015,
abstract = {The reaction of 1-(2-pyridylazo)-2-acenaphthequinol (PAAL) with cobalt acetate in CHCl3gave the complex of Co(PAAL-H)2(1), and (PAAL + H)2[CoCl4]{\textperiodcentered}2H2O (2) was isolated in the same system with ultraviolet light irradiation. Structures of both compounds were determined by X-Ray diffraction. The PAAL ligand was deprotonated in 1, but it further protonated on N position at pyridine group in 2 and form cations. The spectra of these two compounds were also studied, as well as the fluorescence properties. Also, the redox property of 1 was preliminary investigated by cyclic voltammogram.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1016/j.molstruc.2016.12.061},
eprint = {1502.03167},
file = {:Users/lukashanincik/Documents/School/BP/1502.03167.pdf:pdf},
isbn = {9780874216561},
issn = {00222860},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Jaderberg2016,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
doi = {10.1016/j.adolescence.2008.01.003},
eprint = {1608.05343},
file = {:Users/lukashanincik/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - 2016 - Decoupled Neural Interfaces using Synthetic Gradients.pdf:pdf},
isbn = {1836-9553},
issn = {1938-7228},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
url = {http://arxiv.org/abs/1608.05343},
volume = {1},
year = {2016}
}
@article{Li2017,
abstract = {Motivation: Mass cytometry or CyTOF is an emerging technology for high-dimensional multiparameter single cell analysis that overcomes many limitations of fluorescence-based flow cytometry. New methods for analyzing CyTOF data attempt to improve automation, scalability, performance and interpretation of data generated in large studies. Assigning individual cells into discrete groups of cell types (gating) involves time-consuming sequential manual steps, untenable for larger studies. Results: We introduce DeepCyTOF, a standardization approach for gating, based on deep learning techniques. DeepCyTOF requires labeled cells from only a single sample. It is based on domain adaptation principles and is a generalization of previous work that allows us to calibrate between a target distribution and a source distribution in an unsupervised manner. We show that DeepCyTOF is highly concordant (98{\%}) with cell classification obtained by individual manual gating of each sample when applied to a collection of 16 biological replicates of primary immune blood cells, even when measured across several instruments. Further, DeepCyTOF achieves very high accuracy on the semi-automated gating challenge of the FlowCAP-I competition as well as two CyTOF datasets generated from primary immune blood cells: (i) 14 subjects with a history of infection with West Nile virus (WNV), (ii) 34 healthy subjects of different ages. We conclude that deep learning in general, and DeepCyTOF specifically, offers a powerful computational approach for semi-automated gating of CyTOF and flow cytometry data. Availability and implementation: Our codes and data are publicly available at https://github.com/KlugerLab/deepcytof.git. Contact: yuval.kluger@yale.edu. Supplementary information: Supplementary data are available at Bioinformatics online.},
author = {Li, Huamin and Shaham, Uri and Stanton, Kelly P. and Yao, Yi and Montgomery, Ruth R. and Kluger, Yuval},
doi = {10.1093/bioinformatics/btx448},
file = {:Users/lukashanincik/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - Gating mass cytometry data by deep learning(2).pdf:pdf},
isbn = {1367-4811 (Electronic)1367-4803 (Linking)},
issn = {13674811},
journal = {Bioinformatics (Oxford, England)},
number = {21},
pages = {3423--3430},
pmid = {29036374},
title = {{Gating mass cytometry data by deep learning}},
volume = {33},
year = {2017}
}
@article{yann1998mnist,
  title={The MNIST database of handwritten digits},
  author={Yann, LeCun and Corinna, Cortes and Christopher, JB},
  journal={URL http://yhann.lecun.com/exdb/mnist},
  year={1998}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68$\backslash${\%} accuracy on CIFAR-100 test set without multiple test or ensemble.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
doi = {10.1186/1757-1146-1-S1-O22},
eprint = {1505.00853},
file = {:Users/lukashanincik/Documents/School/BP/Sources/ReLU.pdf:pdf},
isbn = {0966-6362},
issn = {1757-1146},
mendeley-groups = {NN acceleration and improvement},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}
@article{Targ2016,
abstract = {Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.},
archivePrefix = {arXiv},
arxivId = {1603.08029},
author = {Targ, Sasha and Almeida, Diogo and Lyman, Kevin},
doi = {10.13005/ojc/300332},
eprint = {1603.08029},
file = {:Users/lukashanincik/Documents/School/BP/Sources/1603.08029.pdf:pdf},
isbn = {9781615671458},
issn = {01932527},
mendeley-groups = {Residual networks},
pages = {1--7},
title = {{Resnet in Resnet: Generalizing Residual Architectures}},
url = {http://arxiv.org/abs/1603.08029},
year = {2016}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:Users/lukashanincik/Documents/School/BP/Sources/IdentityMappingInDeepResNet.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Residual networks},
pages = {630--645},
pmid = {23554596},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Roubalova1934,
abstract = {Recent technological developments in high-dimensional flow cytometry and mass cytometry (CyTOF) have made it possible to detect expression levels of dozens of protein markers in thousands of cells per second, allowing cell populations to be characterized in unprecedented detail. Traditional data analysis by "manual gating" can be inefficient and unreliable in these high-dimensional settings, which has led to the development of a large number of automated analysis methods. Methods designed for unsupervised analysis use specialized clustering algorithms to detect and define cell populations for further downstream analysis. Here, we have performed an up-to-date, extensible performance comparison of clustering methods for high-dimensional flow and mass cytometry data. We evaluated methods using several publicly available data sets from experiments in immunology, containing both major and rare cell populations, with cell population identities from expert manual gating as the reference standard. Several methods performed well, including FlowSOM, X-shift, PhenoGraph, Rclusterpp, and flowMeans. Among these, FlowSOM had extremely fast runtimes, making this method well-suited for interactive, exploratory analysis of large, high-dimensional data sets on a standard laptop or desktop computer. These results extend previously published comparisons by focusing on high-dimensional data and including new methods developed for CyTOF data. R scripts to reproduce all analyses are available from GitHub (https://github.com/lmweber/cytometry-clustering-comparison), and pre-processed data files are available from FlowRepository (FR-FCM-ZZPH), allowing our comparisons to be extended to include new clustering methods and reference data sets.},
author = {Weber, Lukas M. and Robinson, Mark D.},
doi = {10.1002/cyto.a.23030},
file = {:Users/lukashanincik/Documents/School/BP/Sources/Comparison of clustering methods for high-dimensional single-cell flow and mass cytometry data.pdf:pdf},
isbn = {1552-4930},
issn = {15524930},
journal = {Cytometry Part A},
keywords = {CyTOF,F1 score,bioinformatics,cell populations,clustering,flow cytometry,high-dimensional,manual gating,mass cytometry,single-cell},
mendeley-groups = {Cytometricy data classification},
number = {12},
pages = {1084--1096},
pmid = {27992111},
title = {{Comparison of clustering methods for high-dimensional single-cell flow and mass cytometry data}},
volume = {89},
year = {2016}
}
@book{Goh1995,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}
@article{sasaki2007truth,
  title={The truth of the F-measure},
  author={Sasaki, Yutaka and others},
  journal={Teach Tutor mater},
  volume={1},
  number={5},
  pages={1--5},
  year={2007}
}
@inproceedings{Wu2017,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{Aksoy,
author = {Aksoy, Selim and Haralick, Robert M},
file = {:Users/lukashanincik/Documents/School/BP/Sources/FeatureScaling.pdf:pdf},
keywords = {feature normalization,image,image similarity,likelihood ratio,minkowsky metric,retrieval},
mendeley-groups = {Cytometricy data classification,NN acceleration and improvement},
number = {October 2000},
title = {{Feature Normalization and Likelihood-based Similarity Measures for Image Retrieval}}
}
@article{HilbSpace,
  title = {Statistical distance and Hilbert space},
  author = {Wootters, W. K.},
  journal = {Phys. Rev. D},
  volume = {23},
  issue = {2},
  pages = {357--362},
  numpages = {0},
  year = {1981},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.23.357},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.23.357}
}
@article{Alvarez2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.6251v2},
author = {Alvarez, Mauricio A and Sheffield, The and Neuroscience, Translational},
eprint = {arXiv:1106.6251v2},
file = {:Users/lukashanincik/Documents/School/BP/Sources/reproducing{\_}kernel.pdf:pdf},
mendeley-groups = {Cytometricy data classification},
pages = {1--37},
title = {{Kernels for Vector-Valued Functions : a Review}},
year = {2012}
}
@article{formNorm,
title = "The Frobenius norm and the commutator",
journal = "Linear Algebra and its Applications",
volume = "429",
number = "8",
pages = "1864 - 1885",
year = "2008",
issn = "0024-3795",
doi = "https://doi.org/10.1016/j.laa.2008.05.020",
url = "http://www.sciencedirect.com/science/article/pii/S0024379508002772",
author = "Albrecht Böttcher and David Wenzel",
keywords = "Commutator, Frobenius norm, Unitarily invariant norm",
abstract = "In an earlier paper we conjectured an inequality for the Frobenius norm of the commutator of two matrices. This conjecture was recently proved by Seak-Weng Vong and Xiao-Qing Jin and independently also by Zhiqin Lu. We here give a completely different proof of this inequality, prove some related results, and embark on the corresponding question for unitarily invariant norms."
}